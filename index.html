<!DOCTYPE html>

<html>

<head>
  <meta charset="utf-8">
  <title>Coded Portraits</title>
  <link rel="stylesheet" type="text/css" href="style.css">
  <script
  src="https://code.jquery.com/jquery-3.6.0.min.js"
  integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4="
  crossorigin="anonymous"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="favicon.ico">
</head>


  <body>
  	<section class="header">
  		<img class="main-img-dsk" src="images/1-title-bg.jpg" />
      <img class="main-img-mbl" src="images/1-title-bg-mobile.jpg" />
  		<h1>Coded Portraits</h1>
  		<p><span class="space"></span>Towards understanding, appropriating, and reclaiming machine vision</p>
  	</section>
  	
  	<section class="sec-2 black">
  		<h1 class="sec-2-text">This is what I look like to a <span class="yellow">machine</span>.</h1>
  			<div class="img-box-right">
  				<div class="blank">
  				</div>
  				<div class="img-wrap">
		  			<img src="images/2-grid.png" />
		  			<p class="caption">Each of these images is the result of a machine asking—and answering—the question, “what is a person?”</p>
	  			</div>
  			</div>
  	</section>

  	<section class="sec-3 white">
  		<h1 class="sec-3-text">These ways of seeing embody their own kinds of <span class="blue">logic</span>.</h1>
  			<!-- 1 -->
  			<div class="img-box-right">
  				<div class="blank">
  				</div>
  				<div class="img-wrap">
		  			<img src="images/3-classification.png" />
		  			<p class="caption">In the logic of this algorithm, I am seen as a series of boxes that can be recognized and named.</p>
	  			</div>
  			</div>

  			<!-- 2 -->
  			<div class="img-box-left">
  				<div class="img-wrap">
		  			<img src="images/4-emotion.png" />
		  			<p class="caption">This logic determines that I am a group of points defined in relation to one another, which can be used to guess my emotional state.</p>
	  			</div>
	  			<div class="blank">
  				</div>
  			</div>

  			<!-- 3 -->
  			<div class="img-box-right">
  				<div class="blank">
  				</div>
  				<div class="img-wrap">
		  			<img src="images/5-segmentation.png" />
		  			<p class="caption">And in the logic that informs this way of seeing, I am a series of separate pieces—hair, left eye, right eye, upper lip, and so on—that can be broken apart and examined individually.</p>
	  			</div>
  			</div>

  			<!-- 4 -->
  			<div class="img-box-left">
  				<div class="img-wrap">
					<div class="sec-3-img-dsk">
						<img src="images/6-sample code.png" />
					</div>
		  
					<div class="sec-3-img-mbl">
						<img src="images/6-sample code-mobile.jpg" />
					</div>

		  			<p class="caption">But “see” isn’t quite the right word. This image is a much closer representation of how the machine “sees” me. For the world—for us—to be legible to a machine, we need to be translated into numbers.</p>
	  			</div>
	  			<div class="blank">
  				</div>
  			</div>

  			<!-- 5 -->
  			<div class="img-box-right">
  				<div class="blank">
  				</div>
  				<div class="img-wrap">
		  			<img src="images/2-grid.png" />
		  			<p class="caption">These portraits, then, are an <span class="blue">interface</span> mediating between the machine’s way of seeing and our own. Their particular aesthetic qualities emerge from a collaboration between scientists and the software they build, as they attempt to make the world and its inhabitants machine-readable.</p>
	  			</div>
  			</div>
  	</section>

  	<section class="sec-4 black">
  		<h1 class="sec-4-text">What if we could <span class="yellow">re-code</span> these images with different kinds of logic?</h1>
  			<div class="floater-1">
  				<img src="images/7.1-daintyfunk.jpg" />
	  			<p class="caption">@daintyfunk, CV <i>Dazzle, the clown</i>, 2020</p>
	  		</div>

	  		<div class="three-split" >
	  			<div class="img-wrap" >
	  				<img src="images/7.5-redstar.jpg" />
	  				<p class="caption">Wendy Red Star, Déaxitchish/<i>Pretty Eagle</i>, 2014</p>
	  			</div>
	  			
	  			<div class="captions">
	  				<p>1<br>Momtaza Mehri, “The Beautiful Ones,” <i>Real Life</i>, 2017</p>
	  				<br>
	  				<p>2<br>
<span class="list-name">Joy Buolamwini</span>
<span class="list-name">Stephanie Dinkins</span>
<span class="list-name">Dainty Funk (Maud Acheampong)</span>
<span class="list-name">Shawné Michaelain Holloway</span>
<span class="list-name">Wendy Red Star</span>
<span class="list-name">Olivia M. Ross</span>
<span class="list-name">Stephanie Syjuco</span></p>
				</div>

				<div class="text">
					<p>This question is the seed for a series of self-portraits exploring the tensions between automated and human ways of seeing and knowing. </p>
					<p>As a cis white woman, I’m far from an expert on the dangers and possibilities of visibility. For marginalized and racialized communities, visibility <i>and</i> invisibility can carry great risk. And recognition can be powerful. As poet Momtaza Mehri writes, “we devise and circulate our own nomenclature. <span class="yellow">Our own ways of being seen</span>.”<sup>1</sup></p>
					<p>This work is inspired by and in dialogue with femme and nonbinary Black, Indigenous, and Filipinx artists<sup>2</sup> who create <span class="yellow">re-coded</span> portraits—ones that go beyond camouflage and, instead, claim space and complexity. </p>
					<p>Below, I propose a framework for understanding, appropriating, and reclaiming what Joy Buolamwini has termed “the coded gaze.” </p>
				</div>
	  		</div>

	  		<div class="img-block-2" >
	  			<img src="images/7.2-syjuco.jpg" />
	  			<p class="caption">	Stephanie Syjuco, <i>Cargo Cults (Cover-Up)</i>, 2016 </p>
	  		</div>

	  		<div class="img-block-3">
	  			<img src="images/7.4-ross.gif" />
	  			<p class="caption">Olivia M. Ross, <i>@</i>cyberdoula, 2020</p>
	  		</div>

	  		<div class="img-block-4">
	  			<img src="images/7.3-buolamwini.jpg" />
	  			<p class="caption">Joy Buolamwini, still from <i>The Coded Gaze: Unmasking Algorithmic Bias</i>, 2016</p>
	  		</div>

  	</section>

  	<section class="sec-5 white">
  		<h1 class="sec-5-text">Experiments in re-coding</h1>

  		<div class="sec-5-img-dsk">
  			<img src="images/8-grid2.jpg" />
  		</div>

      <div class="sec-5-img-mbl">
        <img src="images/8-grid2-mobile.jpg" />
      </div>

  		<div class="sec-5-txt">
  			<p>What would it mean to recreate these invasive forms of vision? What ways of knowing are illegible or inaccessible to a machine? In what ways do I want to be known, to myself and to others?</p>
			<p>These experiments are a dialogue between my original photograph, the ways it was “seen” and interpreted by a machine, and my response to the gaps and tensions that this “seeing” exposed.</p>
			<p>Ultimately, the re-coded portraits conceal as well as reveal: the altered images are <span class="blue">no longer recognizable</span> as human to facial recognition algorithms.</p>
		</div>
  	</section>

  	<section class="sec-6 black">
  		<h1 class="sec-6-text">A framework for re-coding</h1>

  		<div class="square">
  			<div class="one">
  				<h1>1.</h1>
  				<p>Select a photograph.</p>
  			</div>
  			<div class="three">
  				<h1>3.</h1>
  				<p>Choose a re-coding prompt.</p>
  			</div>
  			<div class="two">
  				<h1>2.</h1>
  				<p>Choose a type of automated recognition to explore.</p>
  			</div>
  			<div class="four">
  				<h1>4.</h1>
  				<p>Re-create the way a machine sees your image. Using the prompt as a guide, add information and context that reflects what <span class="yellow">you</span> consent to share about yourself.</p>
  			</div>
  		</div>
  	</section>

  	<section class="sec-7 white">
  		<div class="sec-7-box">
  			<p>Type of machine vision</p>
  			<h1>Recognition</h1>
  			<p>Re-coding prompt</p>
  			<h1>Self-presentation</h1>
  		</div>

  		<div class="split">
  			<div class="left">
  				<img src="images/9.1-recognition-drawing.jpg" />
  				<p class="caption">With ‘self-presentation’ as a re-coding prompt, I used the areas highlighted by the algorithm to add context about how I chose to represent myself—both in person (my hairstyle and clothing) and in the photograph itself (my expression and pose).</p>
  			</div>
  			<div class="right">
  				<img src="images/9.2-recogntion-model.jpg" />
  				<p class="caption">Model: DenseCap (<a target="_blank" href="https://github.com/agermanidis/densecap-tensorflow" >github</a>)</p>
  			</div>
  		</div>

  		<div class="full-width" >
  			<img class="full-width-dsk" src="images/9.3-recognition-detail.jpg" />
        <img class="full-width-mbl" src="images/9.3-recognition-detail-mobile.jpg" />
  		</div>

  		<div class="sec-7-box">
  			<p>Type of machine vision</p>
  			<h1>Emotion detection</h1>
  			<p>Re-coding prompt</p>
  			<h1>Memory</h1>
  		</div>

  		<div class="split">
  			<div class="left">
  				<img src="images/10.1-emotion-drawing.jpg" />
  				<p class="caption">Using ‘memory’ as a re-coding prompt, I answered the questions the algorithm asks (e.g. which emotions are shown? what is visible?) with my own recollections.</p>
  			</div>
  			<div class="right">
  				<img src="images/10.2-emotion-model.jpg" />
  				<p class="caption">Model: Google Vision AI (<a href="https://cloud.google.com/vision" target="_blank">demo</a>)</p>
  			</div>
  		</div>

  		<div class="full-width" >
  			<img class="full-width-dsk" src="images/10.3-emotion-detail.jpg" />
        <img class="full-width-mbl" src="images/10.3-emotion-detail-mobile.jpg" />
  		</div>

  		<div class="sec-7-box">
  			<p>Type of machine vision</p>
  			<h1>Segmentation</h1>
  			<p>Re-coding prompt</p>
  			<h1>Sensory</h1>
  		</div>

  		<div class="split">
  			<div class="left">
  				<img src="images/11.1-segmentation-drawing.jpg" />
  				<p class="caption">To re-code the form of machine vision that breaks my face apart into separate, named pieces, I used ‘sensory’ as a prompt to augment the legend with sense memories specific to each of my body’s parts.</p>
  			</div>
  			<div class="right">
  				<img src="images/11.2-segmentatin-model.jpg" />
  				<p class="caption">Model: Face-Parser (<a href="https://github.com/agermanidis/face-parsing.PyTorch" target="_blank">github</a>)</p>
  			</div>
  		</div>

  		<div class="full-width" >
  			<img class="full-width-dsk" src="images/11.3-segmentation-detail.jpg" />
        <img class="full-width-mbl" src="images/11.3-segmentation-detail-mobile.jpg" />
  		</div>

  	</section>

  	<section class="sec-8 black">
  		<div class="top-bit">
	  		<h1 class="sec-8-text">About</h1>
	  		<div class="sec-8-txt">This project was created by <a href="https://livia-foldes.com/" target="_blank">Livia Foldes</a>. It was developed in collaboration with Jasmin Liang and Franziska Mack, with generous feedback from Shannon Mattern, Melissa Friedling, and Richard The.</div>
  		</div>

  		<div class="bib">
  			<h1 class="sec-8-text">Bibliography</h1>
  			<div class="bib-split">
  				<p class="entry">
  					<span class="yellow" >Agostinho, Daniela</span>. “Chroma Key Dreams: Algorithmic Visibility, Fleshy Images and Scenes of Recognition.” <i>Philosophy of Photography</i>, vol. 9, no. 2, 1 Oct. 2018, pp. 131–155, 10.1386/pop.9.2.131_1.
  				</p>
  				<p class="entry">
					<span class="yellow" >Ajana, Btihaj</span>. <i>Governing through Biometrics : The Biopolitics of Identity</i>. Erscheinungsort Nicht Ermittelbar, Verlag Nicht Ermittelbar, 2013.
				</p>
  				<p class="entry">
					<span class="yellow" >Anderson, Steve F</span>. <i>Technologies of Vision: The War between Data and Images</i>. Cambridge, Massachusetts, The Mit Press, 2017.
				</p>
				<p class="entry">
					<span class="yellow" >Browne, Simone</span>. <i>Dark Matters: On the Surveillance of Blackness</i>. Durham, Duke University Press, 2015.
				</p>
				<p class="entry">
					<span class="yellow" >Daub, Adrian</span>. “<a href="https://longreads.com/2018/10/03/the-return-of-the-face/" target="_blank">The Return of the Face</a>.” <i>Longreads</i>, 3 Oct. 2018.
				</p>
				<p class="entry">
					<span class="yellow" >Campt, Tina M</span>. <i>Listening to Images</i>. Durham ; London, Duke University Press, 2017.
				</p>
				<p class="entry">
					<span class="yellow" >Crawford, Kate, and Roel Dobbe</span>. <i>AI Now 2019 Report</i>. AI Now Institute, 2019.
				</p>
				<p class="entry">
					<span class="yellow" >Lehmann, Claire</span>. “<a href="https://www.canopycanopycanopy.com/contents/color-goes-electric" target="_blank">Color Goes Electric</a>.” <i>Triple Canopy</i>, 31 May 2016.
				</p>
				<p class="entry">
					<span class="yellow" >House, Brian</span>. “<a href="https://urbanomnibus.net/2019/05/stalking-smart-city/" target="_blank">Stalking the Smart City</a>.” <i>Urban Omnibus</i>, 2 May 2019.
				</p>
				<p class="entry">
					<span class="yellow" >Levin, Boaz, and Vera Tollmann</span>. “<a href="https://archive.transmediale.de/content/bunker-face" target="_blank">Bunker-Face</a>.” <i>Transmediale.De</i>, 2018.
				</p>
				<p class="entry">
					<span class="yellow" >Mattern, Shannon</span>. “<a href="https://placesjournal.org/article/all-eyes-on-the-border/" target="_blank">All Eyes on the Border</a>.” <i>Places Journal</i>, no. 2018, 25 Sept. 2018.
				</p>
				<p class="entry">
					<span class="yellow" >Mehri, Momtaza</span>. “<a href="https://reallifemag.com/the-beautiful-ones/" target="_blank" >The Beautiful Ones</a>.” <i>Real Life</i>, 16 Mar. 2017.
				</p>
				<!-- Break Here -->
				<p class="entry">
					<span class="yellow" >Pipkin, Everest</span>. “<a href="https://unthinking.photography/articles/on-lacework" target="_blank">On Lacework: Watching an Entire Machine-Learning Dataset</a>.” <i>Unthinking Photography</i>, July 2020.
				</p>
				<p class="entry">
					<span class="yellow" >Robertson, Hamish, and Joanne Travaglia</span>. “<a href="https://blogs.lse.ac.uk/impactofsocialsciences/2015/10/13/ideological-inheritances-in-the-data-revolution/" target="_blank">Big Data Problems We Face Today Can Be Traced to the Social Ordering Practices of the 19th Century</a>.” <i>Impact of Social Sciences</i>, 13 Oct. 2015.
				</p>
				<p class="entry">
					<span class="yellow" >Roth, Lorna</span>. “Looking at Shirley, the Ultimate Norm: Colour Balance, Image Technologies, and Cognitive Equity.” <i>Canadian Journal of Communication</i>, vol. 34, no. 1, 28 Mar. 2009, 10.22230/cjc.2009v34n1a2196.
				</p>
				<p class="entry">
					<span class="yellow" >Schmitt, Philipp</span>. “<a href="https://unthinking.photography/articles/tunnel-vision" target="_blank">Tunnel Vision</a>.” <i>Unthinking Photography</i>, Apr. 2020.
				</p>
				<p class="entry">
					<span class="yellow" >Sharpe, Christina Elizabeth</span>. <i>In the Wake: On Blackness and Being</i>. Durham, Duke University Press, 2016.
				</p>
				<p class="entry">
					<span class="yellow" >Slevin, Tom</span>. “Vision, Revelation, Violence: Technology and Expanded Perception within Photographic History.” <i>Philosophy of Photography</i>, vol. 9, no. 1, 1 Apr. 2018, pp. 53–70, 10.1386/pop.9.1.53_1.
				</p>
				<p class="entry">
					<span class="yellow" >Steyerl, Hito</span>. “<a href="https://www.e-flux.com/journal/10/61362/in-defense-of-the-poor-image/" target="_blank">In Defense of the Poor Image</a>.” <i>e-flux</i>, 2009.
				</p>
				<p class="entry">
					<span class="yellow" >Sun Kim, Christine</span>. “<a href="https://www.youtube.com/watch?v=tfe479qL8hg" target="_blank">Artist Christine Sun Kim Rewrites Closed Captions</a>.” <i>Pop-Up Magazine</i>, 13 Oct. 2020.
				</p>
				<p class="entry">
					<span class="yellow" >Woodall, Richard</span>. “<a href="https://reallifemag.com/lying-eyes/" target="_blank" >Lying Eyes</a>.” <i>Real Life</i>, 30 Jan. 2020.
				</p>
				<p class="entry"></p>
				<p class="entry"></p>
  				</div>
  			</div>
  	</section>


  </body>

  </html>